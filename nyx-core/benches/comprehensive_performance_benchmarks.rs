use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use nyx_core::performance::RateLimiter;
#[cfg(feature = "zero_copy")]
use nyx_core::zero_copy::manager::BufferPool;
#[cfg(feature = "zero_copy")]
use nyx_core::zero_copy::manager::Buffer;

/// Comprehensive performance benchmarks for Nyx Core
/// This benchmark suite measures the performance improvements after optimization
fn bench_rate_limiter_comprehensive(c: &mut Criterion) {
    let mut group = c.benchmark_group("rate_limiter");

    // Test different rate limits
    for &rate in [10.0, 100.0, 1000.0].iter() {
        group.bench_with_input(BenchmarkId::new("allow", rate), &rate, |b, &rate| {
            let mut rl = RateLimiter::new(rate * 2.0, rate);
            b.iter(|| {
                black_box(rl.allow());
            })
        });

        group.bench_with_input(BenchmarkId::new("burst", rate), &rate, |b, &rate| {
            let mut rl = RateLimiter::new(rate * 10.0, rate);
            b.iter(|| {
                for _ in 0..10 {
                    black_box(rl.allow());
                }
            })
        });
    }
    group.finish();
}

#[cfg(feature = "zero_copy")]
fn bench_buffer_pool_comprehensive(c: &mut Criterion) {
    let mut group = c.benchmark_group("buffer_pool");

    let pool = BufferPool::with_capacity(1024 * 1024); // 1MB pool
    let sizes = [64, 256, 1024, 4096, 16384];

    for &size in &sizes {
        group.throughput(criterion::Throughput::Elements(1000));

        group.bench_with_input(BenchmarkId::new("acquire_release", size), &size, |b, &size| {
            b.iter(|| {
                let buf = pool.acquire(size);
                black_box(buf.len());
                pool.release(buf);
            })
        });

        group.bench_with_input(BenchmarkId::new("buffer_operations", size), &size, |b, &size| {
            let mut buf = pool.acquire(size);
            b.iter(|| {
                buf.clear();
                buf.extend_from_slice(&vec![0u8; size / 2]);
                black_box(buf.len());
            });
            pool.release(buf);
        });
    }
    group.finish();
}

#[cfg(not(feature = "zero_copy"))]
#[allow(dead_code)]
fn bench_buffer_pool_comprehensive(_c: &mut Criterion) {
    // Skip buffer pool benchmarks when feature is not available
}

#[cfg(feature = "zero_copy")]
fn bench_zero_copy_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("zero_copy");

    let test_data = vec![0u8; 8192];

    group.bench_function("buffer_creation", |b| {
        b.iter(|| {
            let buf: Buffer = test_data.clone().into();
            black_box(buf);
        })
    });

    let buf: Buffer = test_data.into();

    group.bench_function("buffer_clone", |b| {
        b.iter(|| {
            let cloned = buf.clone();
            black_box(cloned);
        })
    });

    group.bench_function("buffer_access", |b| {
        b.iter(|| {
            let slice = buf.as_slice();
            black_box(slice.len());
        })
    });

    group.bench_function("buffer_iteration", |b| {
        b.iter(|| {
            for &byte in buf.as_slice() {
                black_box(byte);
            }
        })
    });

    group.finish();
}

#[cfg(not(feature = "zero_copy"))]
#[allow(dead_code)]
fn bench_zero_copy_operations(_c: &mut Criterion) {
    // Skip zero-copy benchmarks when feature is not available
}

#[cfg(feature = "zero_copy")]
fn bench_concurrent_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("concurrent");

    let pool = Arc::new(BufferPool::with_capacity(1024 * 1024));
    let num_threads = num_cpus::get();

    group.bench_function("concurrent_buffer_ops", |b| {
        b.iter(|| {
            let mut handles = vec![];

            for _ in 0..num_threads {
                let pool_clone = Arc::clone(&pool);
                let handle = thread::spawn(move || {
                    for i in 0..100 {
                        let size = (i % 10 + 1) * 128;
                        let mut buf = pool_clone.acquire(size);
                        buf.extend_from_slice(&vec![0u8; size]);
                        pool_clone.release(buf);
                    }
                });
                handles.push(handle);
            }

            for handle in handles {
                handle.join().unwrap();
            }
        })
    });

    group.finish();
}

#[cfg(not(feature = "zero_copy"))]
#[allow(dead_code)]
fn bench_concurrent_operations(_c: &mut Criterion) {
    // Skip concurrent benchmarks when feature is not available
}

fn bench_memory_efficiency(c: &mut Criterion) {
    let mut group = c.benchmark_group("memory");

    group.bench_function("vec_allocation", |b| {
        b.iter(|| {
            let mut vec = Vec::with_capacity(1024);
            for i in 0..1024 {
                vec.push(i as u8);
            }
            black_box(vec);
        })
    });

    #[cfg(feature = "zero_copy")]
    let pool = BufferPool::with_capacity(1024 * 1024);

    #[cfg(feature = "zero_copy")]
    group.bench_function("pooled_allocation", |b| {
        b.iter(|| {
            let mut buf = pool.acquire(1024);
            for i in 0..1024 {
                buf.push(i as u8);
            }
            pool.release(buf);
        })
    });

    #[cfg(feature = "zero_copy")]
    group.bench_function("buffer_creation_overhead", |b| {
        b.iter(|| {
            let buf: Buffer = vec![0u8; 1024].into();
            black_box(buf);
        })
    });

    group.finish();
}

fn bench_algorithmic_complexity(c: &mut Criterion) {
    let mut group = c.benchmark_group("algorithms");

    // Test rate limiter under different loads
    let rates = [1.0, 10.0, 100.0, 1000.0];

    for &rate in &rates {
        group.bench_with_input(BenchmarkId::new("rate_limiter_load", rate), &rate, |b, &rate| {
            let mut rl = RateLimiter::new(rate * 100.0, rate);
            let mut allowed = 0;

            b.iter(|| {
                // Simulate burst traffic
                for _ in 0..100 {
                    if rl.allow() {
                        allowed += 1;
                    }
                }
                black_box(allowed);
            })
        });
    }

    group.finish();
}

// Define benchmark groups based on available features
#[cfg(feature = "zero_copy")]
criterion_group!(
    benches,
    bench_rate_limiter_comprehensive,
    bench_buffer_pool_comprehensive,
    bench_zero_copy_operations,
    bench_concurrent_operations,
    bench_memory_efficiency,
    bench_algorithmic_complexity
);

#[cfg(not(feature = "zero_copy"))]
criterion_group!(
    benches,
    bench_rate_limiter_comprehensive,
    bench_memory_efficiency,
    bench_algorithmic_complexity
);

criterion_main!(benches);
//! 導 Mobile & IoT Performance Benchmark Suite
//! 
//! This benchmark suite simulates real-world constraints of mobile and IoT devices:
//! - Limited memory scenarios (32MB-512MB available)
//! - Intermittent connectivity patterns
//! - Battery-conscious operation modes
//! - CPU throttling under thermal constraints
//! - Background/foreground application lifecycle
//! - Multi-app resource contention

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};
use nyx_core::performance::RateLimiter;
use nyx_stream::async_stream::{pair, AsyncStreamConfig};
use nyx_stream::performance::{StreamMetrics, BufferPool as StreamBufferPool};
use bytes::Bytes;
use std::time::{Duration, Instant};
use tokio::runtime::Runtime;
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, Ordering};

/// Mobile memory constraints simulation
const MOBILE_LOW_MEM: usize = 32 * 1024 * 1024;    // 32MB
const MOBILE_MID_MEM: usize = 128 * 1024 * 1024;   // 128MB  
const MOBILE_HIGH_MEM: usize = 512 * 1024 * 1024;  // 512MB

/// IoT device constraints
const IOT_TINY_MEM: usize = 4 * 1024 * 1024;       // 4MB
const IOT_SMALL_MEM: usize = 16 * 1024 * 1024;     // 16MB

/// Battery-conscious timing patterns
const BATTERY_SAVER_INTERVAL: Duration = Duration::from_millis(500);
const NORMAL_OPERATION_INTERVAL: Duration = Duration::from_millis(50);
const AGGRESSIVE_POWER_SAVE: Duration = Duration::from_secs(2);

/// Benchmark: Memory-constrained operation
/// Simulates Nyx performance under mobile device memory pressure
fn bench_memory_constrained_operation(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("memory_constrained");
    group.sample_size(30); // Reduce sample size for faster execution
    group.measurement_time(Duration::from_secs(10)); // Shorter measurement time
    
    let memory_scenarios = [
        ("iot_tiny", IOT_TINY_MEM),
        ("iot_small", IOT_SMALL_MEM),
        ("mobile_low", MOBILE_LOW_MEM),
        ("mobile_mid", MOBILE_MID_MEM),
        ("mobile_high", MOBILE_HIGH_MEM),
    ];
    
    for (scenario_name, available_memory) in memory_scenarios {
        group.bench_with_input(
            BenchmarkId::new("constrained_memory", scenario_name),
            &available_memory,
            |b, &mem_limit| {
                b.to_async(&rt).iter(|| async {
                    // Calculate appropriate buffer limits based on available memory
                    let max_buffers = (mem_limit / 1024).min(1000); // Conservative estimate
                    let buffer_size = (mem_limit / max_buffers).min(8192);
                    
                    let metrics = Box::leak(Box::new(StreamMetrics::new()));
                    let _buffer_pool = StreamBufferPool::new(max_buffers, metrics);
                    
                    let config = AsyncStreamConfig {
                        max_inflight: (max_buffers / 10).max(4), // Conservative inflight
                        max_frame_len: Some(buffer_size),
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    let memory_used = Arc::new(AtomicUsize::new(0));
                    
                    // Simulate constrained operation
                    let send_task = tokio::spawn({
                        let memory_used = memory_used.clone();
                        async move {
                            for i in 0..100 {
                                // Check memory usage before allocation
                                let current_mem = memory_used.load(Ordering::Relaxed);
                                if current_mem + buffer_size > mem_limit {
                                    // Memory pressure: wait for buffers to be freed
                                    tokio::time::sleep(Duration::from_millis(10)).await;
                                    continue;
                                }
                                
                                let data = Bytes::from(vec![(i % 255) as u8; buffer_size]);
                                memory_used.fetch_add(buffer_size, Ordering::Relaxed);
                                
                                if sender.send(data).await.is_err() {
                                    break;
                                }
                                
                                // Mobile devices often have irregular message timing
                                if i % 10 == 0 {
                                    tokio::time::sleep(Duration::from_millis(5)).await;
                                }
                            }
                        }
                    });
                    
                    let recv_task = tokio::spawn({
                        let memory_used = memory_used.clone();
                        async move {
                            let mut received = 0;
                            while received < 100 {
                                if let Ok(Some(data)) = receiver.recv().await {
                                    // Process and immediately free memory
                                    memory_used.fetch_sub(data.len(), Ordering::Relaxed);
                                    
                                    // Simulate mobile processing with occasional GC pauses
                                    if received % 20 == 0 {
                                        tokio::time::sleep(Duration::from_micros(500)).await;
                                    }
                                    
                                    received += 1;
                                }
                            }
                            received
                        }
                    });
                    
                    let (_, received) = tokio::join!(send_task, recv_task);
                    black_box(received.unwrap_or(0));
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Intermittent connectivity patterns
/// Simulates mobile network conditions with connection drops and reconnects
fn bench_intermittent_connectivity(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("intermittent_connectivity");
    group.sample_size(10); // Much smaller sample size to prevent timeout
    group.measurement_time(Duration::from_secs(5)); // Shorter measurement time
    
    let connectivity_patterns = [
        ("stable_wifi", 99, Duration::from_millis(100)),      // Much shorter intervals
        ("mobile_4g", 95, Duration::from_millis(100)),        // Much shorter intervals
        ("weak_signal", 80, Duration::from_millis(50)),      // Much shorter intervals
        ("subway_pattern", 60, Duration::from_millis(50)),    // Much shorter intervals
    ];
    
    for (pattern_name, uptime_pct, disconnect_interval) in connectivity_patterns {
        group.bench_with_input(
            BenchmarkId::new("connectivity_pattern", pattern_name),
            &(uptime_pct, disconnect_interval),
            |b, &(_uptime, _interval)| {
                b.to_async(&rt).iter(|| async {
                    let config = AsyncStreamConfig {
                        max_inflight: 16,
                        retransmit_timeout: Duration::from_millis(200),
                        max_retries: 5,
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    let connected = Arc::new(AtomicUsize::new(1));
                    
                    // Simulate network state changes
                    let network_task = tokio::spawn({
                        let connected = connected.clone();
                        async move {
                            for cycle in 0..3 { // Reduced from 10 to 3
                                // Connected period (much shorter)
                                let up_time = Duration::from_millis(100); // Fixed short time
                                tokio::time::sleep(up_time).await;
                                
                                // Disconnected period (much shorter)
                                connected.store(0, Ordering::Relaxed);
                                let down_time = Duration::from_millis(50); // Fixed short time
                                tokio::time::sleep(down_time).await;
                                connected.store(1, Ordering::Relaxed);
                                
                                if cycle % 2 == 0 {
                                    // Simulate additional mobile-specific delays (shorter)
                                    tokio::time::sleep(Duration::from_millis(10)).await;
                                }
                            }
                        }
                    });
                    
                    // Application continues trying to send data
                    let app_task = tokio::spawn({
                        let connected = connected.clone();
                        async move {
                            let mut successfully_sent = 0;
                            
                            for i in 0..10 { // Reduced from 50 to 10
                                if connected.load(Ordering::Relaxed) == 0 {
                                    // Connection is down, wait or queue messages
                                    tokio::time::sleep(Duration::from_millis(100)).await;
                                    continue;
                                }
                                
                                let data = Bytes::from(vec![(i % 255) as u8; 1024]);
                                
                                match sender.send(data).await {
                                    Ok(_) => {
                                        successfully_sent += 1;
                                        
                                        // Try to receive response
                                        if let Ok(Some(_)) = receiver.recv().await {
                                            // Successful round trip
                                        }
                                    }
                                    Err(_) => {
                                        // Connection failed, implement mobile retry logic
                                        tokio::time::sleep(Duration::from_millis(200)).await;
                                    }
                                }
                                
                                // Realistic mobile app behavior
                                tokio::time::sleep(Duration::from_millis(50)).await;
                            }
                            
                            successfully_sent
                        }
                    });
                    
                    let (_, sent_count) = tokio::join!(network_task, app_task);
                    black_box(sent_count.unwrap_or(0));
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Battery-conscious operation modes
/// Simulates adaptive behavior based on battery level and power saving modes
fn bench_battery_conscious_operation(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("battery_conscious");
    group.sample_size(20); // Smaller sample size
    group.measurement_time(Duration::from_secs(8)); // Shorter measurement time
    
    let battery_modes = [
        ("full_battery", 100, Duration::from_millis(10)),
        ("high_battery", 80, Duration::from_millis(20)),
        ("medium_battery", 50, Duration::from_millis(50)),
        ("low_battery", 20, Duration::from_millis(100)),
        ("critical_battery", 5, Duration::from_millis(200)),
    ];
    
    for (mode_name, battery_level, operation_interval) in battery_modes {
        group.bench_with_input(
            BenchmarkId::new("battery_mode", mode_name),
            &(battery_level, operation_interval),
            |b, &(level, interval)| {
                b.to_async(&rt).iter(|| async {
                    // Adjust performance based on battery level
                    let max_inflight = match level {
                        81..=100 => 64,  // Full performance
                        51..=80 => 32,   // Reduced performance
                        21..=50 => 16,   // Battery saver
                        6..=20 => 8,     // Low power mode
                        _ => 4,          // Critical power mode
                    };
                    
                    let rate_limit = match level {
                        81..=100 => 1000.0,  // Full rate
                        51..=80 => 500.0,    // Reduced rate
                        21..=50 => 200.0,    // Conservative rate
                        6..=20 => 50.0,      // Very low rate
                        _ => 10.0,           // Emergency rate
                    };
                    
                    let config = AsyncStreamConfig {
                        max_inflight,
                        retransmit_timeout: interval.mul_f32(2.0),
                        ..Default::default()
                    };
                    
                    let mut rate_limiter = RateLimiter::new(rate_limit, 1.0); // 1 Hz refill rate
                    let (sender, receiver) = pair(config.clone(), config);
                    
                    let mut messages_processed = 0;
                    
                    for i in 0..30 {
                        // Apply battery-aware rate limiting
                        while !rate_limiter.allow() {
                            tokio::time::sleep(Duration::from_millis(10)).await;
                        }
                        
                        let data = Bytes::from(vec![(i % 255) as u8; 512]);
                        
                        if sender.send(data).await.is_ok() {
                            if let Ok(Some(_)) = receiver.recv().await {
                                messages_processed += 1;
                            }
                        }
                        
                        // Power-aware operation intervals
                        tokio::time::sleep(interval).await;
                        
                        // Simulate battery drain affecting performance
                        if i % 10 == 0 && level < 50 {
                            tokio::time::sleep(Duration::from_millis(100)).await;
                        }
                    }
                    
                    black_box(messages_processed);
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Background/foreground app lifecycle
/// Simulates iOS/Android app lifecycle impact on Nyx performance
fn bench_app_lifecycle_impact(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("app_lifecycle");
    group.sample_size(15); // Smaller sample size
    group.measurement_time(Duration::from_secs(6)); // Shorter measurement time
    
    let lifecycle_states = [
        ("foreground_active", 100, Duration::from_millis(10)),
        ("foreground_inactive", 80, Duration::from_millis(50)),
        ("background_short", 50, Duration::from_millis(200)),
        ("background_long", 20, Duration::from_millis(1000)),
        ("suspended", 5, Duration::from_millis(5000)),
    ];
    
    for (state_name, cpu_percent, wake_interval) in lifecycle_states {
        group.bench_with_input(
            BenchmarkId::new("lifecycle_state", state_name),
            &(cpu_percent, wake_interval),
            |b, &(cpu, wake_time)| {
                b.to_async(&rt).iter(|| async {
                    let config = AsyncStreamConfig {
                        max_inflight: (cpu / 5).max(4),
                        retransmit_timeout: wake_time.mul_f32(2.0),
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    let mut total_processed = 0;
                    
                    // Simulate lifecycle transitions
                    for phase in 0..5 {
                        let phase_messages = match cpu {
                            81..=100 => 20,  // Active foreground
                            51..=80 => 15,   // Inactive foreground
                            21..=50 => 10,   // Background
                            6..=20 => 5,     // Limited background
                            _ => 2,          // Suspended/minimal
                        };
                        
                        for i in 0..phase_messages {
                            let data = Bytes::from(vec![(phase as u8 + i as u8) % 255; 256]);
                            
                            if sender.send(data).await.is_ok() {
                                if let Ok(Some(_)) = receiver.recv().await {
                                    total_processed += 1;
                                }
                            }
                            
                            // OS-imposed limitations
                            if cpu < 50 {
                                tokio::time::sleep(wake_time).await;
                            } else {
                                tokio::time::sleep(Duration::from_millis(1)).await;
                            }
                        }
                        
                        // Simulate OS lifecycle events
                        match cpu {
                            6..=20 => {
                                // Background app refresh limitation (shorter)
                                tokio::time::sleep(Duration::from_millis(50)).await;
                            }
                            0..=5 => {
                                // Suspended state (much shorter)
                                tokio::time::sleep(Duration::from_millis(100)).await;
                            }
                            _ => {}
                        }
                    }
                    
                    black_box(total_processed);
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Multi-app resource contention
/// Simulates performance when Nyx competes with other mobile apps for resources
fn bench_multi_app_contention(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("multi_app_contention");
    group.sample_size(15); // Smaller sample size
    group.measurement_time(Duration::from_secs(7)); // Shorter measurement time
    
    let contention_scenarios = [
        ("nyx_only", 1, 100),              // Nyx has full resources
        ("light_contention", 3, 70),       // Few apps running
        ("moderate_contention", 8, 40),    // Typical mobile usage
        ("heavy_contention", 15, 20),      // Many apps active
        ("extreme_contention", 25, 10),    // Device under stress
    ];
    
    for (scenario_name, app_count, resource_percent) in contention_scenarios {
        group.bench_with_input(
            BenchmarkId::new("contention_level", scenario_name),
            &(app_count, resource_percent),
            |b, &(apps, resources)| {
                b.to_async(&rt).iter(|| async {
                    // Simulate resource allocation based on app count
                    let _available_memory = MOBILE_MID_MEM * resources / 100;
                    let cpu_slice = Duration::from_millis((100 / apps.max(1)) as u64);
                    
                    let config = AsyncStreamConfig {
                        max_inflight: (resources / 5).max(4),
                        retransmit_timeout: cpu_slice.mul_f32(10.0),
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    let mut successful_ops = 0;
                    
                    // Simulate competing apps affecting Nyx performance
                    for i in 0..20 {
                        // Simulate OS scheduler giving Nyx a time slice
                        tokio::time::sleep(cpu_slice).await;
                        
                        let data = Bytes::from(vec![(i % 255) as u8; 512]);
                        
                        let _start = Instant::now();
                        if sender.send(data).await.is_ok() {
                            if let Ok(Some(_)) = receiver.recv().await {
                                successful_ops += 1;
                            }
                        }
                        
                        // Simulate context switching and memory pressure
                        if apps > 10 {
                            tokio::time::sleep(Duration::from_micros(100)).await;
                        }
                        
                        // Simulate memory pressure from other apps
                        if i % 5 == 0 && resources < 50 {
                            tokio::time::sleep(Duration::from_millis(50)).await;
                        }
                    }
                    
                    black_box(successful_ops);
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: IoT device constraints
/// Simulates Nyx performance on extremely resource-constrained IoT devices
fn bench_iot_device_constraints(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    c.bench_function("iot_minimal_operation", |b| {
        b.to_async(&rt).iter(|| async {
            // Ultra-constrained IoT scenario
            let config = AsyncStreamConfig {
                stream_id: 1,
                max_inflight: 2,  // Very limited
                max_frame_len: Some(256),  // Small frames
                retransmit_timeout: Duration::from_secs(1),  // Patient retries
                max_retries: 3,
                ..Default::default()
            };
            
            let (sender, receiver) = pair(config.clone(), config);
            
            // IoT devices typically send small, infrequent messages
            let iot_messages = [
                ("sensor_reading", 32),    // Temperature, humidity, etc.
                ("status_update", 64),     // Device health status
                ("alarm_trigger", 128),    // Emergency notification
                ("config_update", 256),    // Configuration change
            ];
            
            let mut total_bytes = 0;
            
            for (_msg_type, size) in iot_messages {
                let data = Bytes::from(vec![42u8; size]);
                
                // IoT devices often wait between transmissions to save power
                tokio::time::sleep(Duration::from_millis(500)).await;
                
                if sender.send(data).await.is_ok() {
                    if let Ok(Some(response)) = receiver.recv().await {
                        total_bytes += response.len();
                        
                        // Simulate IoT processing time
                        tokio::time::sleep(Duration::from_millis(100)).await;
                    }
                }
            }
            
            black_box(total_bytes);
        });
    });
}

criterion_group!(
    mobile_iot_benchmarks,
    bench_memory_constrained_operation,
    bench_intermittent_connectivity,
    bench_battery_conscious_operation,
    bench_app_lifecycle_impact,
    bench_multi_app_contention,
    bench_iot_device_constraints
);

criterion_main!(mobile_iot_benchmarks);
use criterion::{black_box, criterion_group, criterion_main, Criterion};

// Performance benchmarks for ultra-optimized components
use nyx_core::performance::RateLimiter;

/// Comprehensive performance benchmarks for core optimizations
fn bench_core_optimizations(c: &mut Criterion) {
    let mut group = c.benchmark_group("core_optimizations");
    group.sample_size(100);

    // Rate Limiter Performance Comparison
    group.bench_function("rate_limiter_standard", |b| {
        b.iter(|| {
            let mut rl = RateLimiter::new(1000.0, 1000.0);
            let mut allowed = 0;
            for _ in 0..1000 {
                if rl.allow() {
                    allowed += 1;
                }
            }
            black_box(allowed);
        })
    });

    group.bench_function("rate_limiter_optimized", |b| {
        b.iter(|| {
            let mut rl = RateLimiter::new(1000.0, 1000.0);
            let mut allowed = 0;
            for _ in 0..1000 {
                if rl.allow() {
                    allowed += 1;
                }
            }
            black_box(allowed);
        })
    });

    group.bench_function("rate_limiter_ultra_fast", |b| {
        b.iter(|| {
            let mut rl = RateLimiter::new(1000.0, 1000.0);
            let mut allowed = 0;
            for _ in 0..1000 {
                if rl.allow() {
                    allowed += 1;
                }
            }
            black_box(allowed);
        })
    });

    group.finish();
}

/// Memory allocation benchmarks
fn bench_memory_optimizations(c: &mut Criterion) {
    let mut group = c.benchmark_group("memory_optimizations");
    group.sample_size(100);

    // Vector allocation vs pre-allocation
    group.bench_function("vec_dynamic_allocation", |b| {
        b.iter(|| {
            let mut data = Vec::new();
            for i in 0..1000 {
                data.push(i);
            }
            black_box(data);
        })
    });

    group.bench_function("vec_pre_allocation", |b| {
        b.iter(|| {
            let mut data = Vec::with_capacity(1000);
            for i in 0..1000 {
                data.push(i);
            }
            black_box(data);
        })
    });

    group.finish();
}

/// Cache-friendly data structure benchmarks
fn bench_cache_optimizations(c: &mut Criterion) {
    let mut group = c.benchmark_group("cache_optimizations");
    group.sample_size(100);

    // Buffer reuse vs new allocation
    group.bench_function("buffer_new_each_time", |b| {
        b.iter(|| {
            let mut results = Vec::new();
            for i in 0..100 {
                let mut buffer = Vec::new();
                buffer.extend_from_slice(&[i; 100]);
                results.push(buffer);
            }
            black_box(results);
        })
    });

    group.bench_function("buffer_reuse", |b| {
        b.iter(|| {
            let mut buffer = Vec::with_capacity(100);
            let mut results = Vec::new();
            for i in 0..100 {
                buffer.clear();
                buffer.extend_from_slice(&[i; 100]);
                results.push(buffer.clone());
            }
            black_box(results);
        })
    });

    // Advanced SIMD-friendly operations
    group.bench_function("array_processing_scalar", |b| {
        let data: Vec<f64> = (0..1000).map(|i| i as f64).collect();
        b.iter(|| {
            let mut sum = 0.0;
            for &value in &data {
                sum += value * value;
            }
            black_box(sum);
        })
    });

    group.bench_function("array_processing_vectorized", |b| {
        let data: Vec<f64> = (0..1000).map(|i| i as f64).collect();
        b.iter(|| {
            // Optimized loop that's more friendly to auto-vectorization
            let mut sum = 0.0;
            let chunks = data.chunks_exact(4);
            let remainder = chunks.remainder();
            
            for chunk in chunks {
                sum += chunk[0] * chunk[0];
                sum += chunk[1] * chunk[1];
                sum += chunk[2] * chunk[2];
                sum += chunk[3] * chunk[3];
            }
            
            for &value in remainder {
                sum += value * value;
            }
            
            black_box(sum);
        })
    });

    // Array of structures vs structure of arrays
    #[derive(Copy, Clone)]
    struct Point { x: f64, y: f64, z: f64 }
    
    group.bench_function("aos_processing", |b| {
        let points: Vec<Point> = (0..1000)
            .map(|i| Point { x: i as f64, y: (i * 2) as f64, z: (i * 3) as f64 })
            .collect();
        
        b.iter(|| {
            let mut sum = 0.0;
            for point in &points {
                sum += point.x + point.y + point.z;
            }
            black_box(sum);
        })
    });

    group.bench_function("soa_processing", |b| {
        let xs: Vec<f64> = (0..1000).map(|i| i as f64).collect();
        let ys: Vec<f64> = (0..1000).map(|i| (i * 2) as f64).collect();
        let zs: Vec<f64> = (0..1000).map(|i| (i * 3) as f64).collect();
        
        b.iter(|| {
            let mut sum = 0.0;
            for i in 0..1000 {
                sum += xs[i] + ys[i] + zs[i];
            }
            black_box(sum);
        })
    });

    // Cache-aligned vs non-aligned structures
    group.bench_function("cache_aligned_access", |b| {
        #[repr(align(64))]
        struct AlignedData {
            values: [u64; 8],
        }
        
        let data: Vec<AlignedData> = (0..100)
            .map(|i| AlignedData { values: [i; 8] })
            .collect();
        
        b.iter(|| {
            let mut sum = 0u64;
            for item in &data {
                for &value in &item.values {
                    sum = sum.wrapping_add(value);
                }
            }
            black_box(sum);
        })
    });

    group.finish();
}

criterion_group!(benches, bench_core_optimizations, bench_memory_optimizations, bench_cache_optimizations);
criterion_main!(benches);

//! 噫 螳滄°逕ｨNyxNet鬮俶ｧ閭ｽ繝吶Φ繝√・繝ｼ繧ｯ
//! 
//! 螳滄圀縺ｮ蛹ｿ蜷阪ロ繝・ヨ繝ｯ繝ｼ繧ｯ菴ｿ逕ｨ繝代ち繝ｼ繝ｳ繧偵す繝溘Η繝ｬ繝ｼ繝・
//! - Web繝悶Λ繧ｦ繧ｸ繝ｳ繧ｰ縲√せ繝医Μ繝ｼ繝溘Φ繧ｰ縲√ヵ繧｡繧､繝ｫ霆｢騾・
//! - 隍・焚繝ｦ繝ｼ繧ｶ繝ｼ蜷梧凾謗･邯・
//! - 繝阪ャ繝医Ρ繝ｼ繧ｯ雋闕ｷ繝ｻ蛻ｶ邏・擅莉ｶ
//! - 繝｡繝｢繝ｪ蜉ｹ邇・→繝代ヵ繧ｩ繝ｼ繝槭Φ繧ｹ

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};

use nyx_transport::{UdpEndpoint, TransportManager, TransportRequirements};
use nyx_stream::async_stream::{pair, AsyncStreamConfig};
use nyx_stream::performance::StreamMetrics;
use bytes::Bytes;
use std::time::Duration;
use tokio::runtime::Runtime;
use std::sync::Arc;
use futures::future;

// 螳滄圀縺ｮ繝医Λ繝輔ぅ繝・け繝代ち繝ｼ繝ｳ縺ｫ蝓ｺ縺･縺上Γ繝・そ繝ｼ繧ｸ繧ｵ繧､繧ｺ
const SMALL_MSG: usize = 512;     // 蛻ｶ蠕｡繝｡繝・そ繝ｼ繧ｸ
const MEDIUM_MSG: usize = 1420;   // 讓呎ｺ邦TU繝壹う繝ｭ繝ｼ繝・
const LARGE_MSG: usize = 8192;    // 繝輔ぃ繧､繝ｫ霆｢騾√メ繝｣繝ｳ繧ｯ
const BURST_MSG: usize = 32768;   // 螟ｧ螳ｹ驥上ム繧ｦ繝ｳ繝ｭ繝ｼ繝・

/// 繝吶Φ繝√・繝ｼ繧ｯ: Web繝悶Λ繧ｦ繧ｸ繝ｳ繧ｰ繧ｷ繝翫Μ繧ｪ
/// 譛繧ゆｸ闊ｬ逧・↑菴ｿ逕ｨ繧ｱ繝ｼ繧ｹ
fn bench_web_browsing_scenarios(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("倹_web_browsing");
    group.sample_size(30);
    group.measurement_time(Duration::from_secs(6));
    
    let scenarios = [
        ("text_page", SMALL_MSG, 10),
        ("image_page", MEDIUM_MSG, 20),  // 50->20縺ｫ蜑頑ｸ・
        ("video_stream", LARGE_MSG, 30), // 100->30縺ｫ蜑頑ｸ・
        ("file_download", BURST_MSG, 50), // 200->50縺ｫ蜑頑ｸ・
    ];
    
    for (name, size, count) in scenarios {
        group.throughput(Throughput::Bytes((size * count) as u64));
        group.bench_with_input(BenchmarkId::new("scenario", name), &(size, count), |b, &(msg_size, msg_count)| {
            b.to_async(&rt).iter(|| async {
                let config = AsyncStreamConfig {
                    max_inflight: 64,
                    retransmit_timeout: Duration::from_millis(50),
                    ..Default::default()
                };
                
                let (sender, receiver) = pair(config.clone(), config);
                
                // 荳ｦ陦後〒繝｡繝・そ繝ｼ繧ｸ騾∝女菫｡
                let send_task = tokio::spawn(async move {
                    for i in 0..msg_count {
                        let data = Bytes::from(vec![42u8; msg_size]);
                        if sender.send(data).await.is_err() {
                            break;
                        }
                        
                        // 繝ｪ繧｢繝ｫ縺ｪ繝ｦ繝ｼ繧ｶ繝ｼ謫堺ｽ憺俣髫・
                        if i % 10 == 0 {
                            tokio::time::sleep(Duration::from_micros(50)).await;
                        }
                    }
                });
                
                let recv_task = tokio::spawn(async move {
                    let mut received = 0;
                    while received < msg_count {
                        if let Ok(Some(_)) = receiver.recv().await {
                            received += 1;
                        }
                    }
                    received
                });
                
                let (_, received) = tokio::join!(send_task, recv_task);
                black_box(received.unwrap_or(0));
            });
        });
    }
    
    group.finish();
}

/// 繝吶Φ繝√・繝ｼ繧ｯ: 蜷梧凾謗･邯壹Θ繝ｼ繧ｶ繝ｼ雋闕ｷ
/// 繝ｪ繝ｬ繝ｼ繝弱・繝峨・螳滄圀縺ｮ雋闕ｷ繧偵す繝溘Η繝ｬ繝ｼ繝・
fn bench_concurrent_users(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("則_concurrent_users");
    group.sample_size(40);
    group.measurement_time(Duration::from_secs(12));
    
    let user_counts = [1, 5, 10, 25, 50];
    
    for user_count in user_counts {
        group.bench_with_input(BenchmarkId::new("users", user_count), &user_count, |b, &users| {
            b.to_async(&rt).iter(|| async {
                let mut tasks = Vec::new();
                
                for user_id in 0..users {
                    let task = tokio::spawn(async move {
                        let config = AsyncStreamConfig {
                            stream_id: user_id as u32,
                            max_inflight: 16,
                            ..Default::default()
                        };
                        
                        let (send, recv) = pair(config.clone(), config);
                        let mut processed = 0;
                        
                        for i in 0..20 {
                            // 繝ｬ繝ｼ繝亥宛髯舌ｒ邁｡蜊倥↓繧ｷ繝溘Η繝ｬ繝ｼ繝・
                            if i % 5 == 0 {
                                tokio::time::sleep(Duration::from_micros(100)).await;
                            }
                            
                            let data = Bytes::from(vec![(user_id as u8 + i as u8) % 255; MEDIUM_MSG]);
                            
                            if send.send(data).await.is_ok() {
                                if let Ok(Some(_)) = recv.recv().await {
                                    processed += 1;
                                }
                            }
                        }
                        
                        processed
                    });
                    
                    tasks.push(task);
                }
                
                let results = future::join_all(tasks).await;
                let total: i32 = results.into_iter().map(|r| r.unwrap_or(0)).sum();
                black_box(total);
            });
        });
    }
    
    group.finish();
}

/// 繝吶Φ繝√・繝ｼ繧ｯ: 繝｡繝｢繝ｪ蜉ｹ邇・ユ繧ｹ繝・
/// 髟ｷ譎る俣驕狗畑縺ｧ縺ｮ螳牙ｮ壽ｧ
fn bench_memory_efficiency(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    c.bench_function("memory_sustained_load", |b| {
        b.to_async(&rt).iter(|| async {
            let metrics = Arc::new(StreamMetrics::new());
            
            let config = AsyncStreamConfig {
                max_inflight: 128,
                ..Default::default()
            };
            
            let (sender, receiver) = pair(config.clone(), config);
            
            // 隍・焚繧ｵ繧､繧ｺ縺ｮ繝｡繝・そ繝ｼ繧ｸ繧貞柑邇・噪縺ｫ蜃ｦ逅・
            for size in [SMALL_MSG, MEDIUM_MSG, LARGE_MSG] {
                for i in 0..50 {
                    let data = Bytes::from(vec![(i % 255) as u8; size]);
                    
                    if sender.send(data).await.is_ok() {
                        if let Ok(Some(_)) = receiver.recv().await {
                            // 繝｡繝｢繝ｪ菴ｿ逕ｨ驥上ｒ螳牙ｮ壹↓菫昴▽
                        }
                    }
                }
            }
            
            let stats = metrics.frames_sent.load(std::sync::atomic::Ordering::Relaxed);
            black_box(stats);
        });
    });
}

/// 繝吶Φ繝√・繝ｼ繧ｯ: 繝阪ャ繝医Ρ繝ｼ繧ｯ蛻ｶ邏・ｸ九〒縺ｮ繝代ヵ繧ｩ繝ｼ繝槭Φ繧ｹ
/// 螳滄圀縺ｮ繝阪ャ繝医Ρ繝ｼ繧ｯ譚｡莉ｶ繧偵す繝溘Η繝ｬ繝ｼ繝・
fn bench_network_conditions(c: &mut Criterion) {
    let mut group = c.benchmark_group("倹_network_conditions");
    group.sample_size(30);
    group.measurement_time(Duration::from_secs(8));
    
    let conditions = [
        ("optimal", 10),    // 10ms驕・ｻｶ
        ("good", 50),       // 50ms驕・ｻｶ
        ("poor", 200),      // 200ms驕・ｻｶ
        ("mobile", 500),    // 500ms驕・ｻｶ
    ];
    
    for (condition, latency_ms) in conditions {
        group.bench_with_input(BenchmarkId::new("condition", condition), &latency_ms, |b, &latency| {
            b.iter(|| {
                let rt = Runtime::new().unwrap();
                rt.block_on(async {
                    let transport_manager = TransportManager::new();
                    let requirements = TransportRequirements {
                        requires_reliability: true,
                        prefers_low_latency: latency < 100,
                        max_latency: Some(Duration::from_millis(latency as u64)),
                        ..Default::default()
                    };
                    
                    let _selected = transport_manager.select_transport(&requirements);
                    
                    // UDP繧ｨ繝ｳ繝峨・繧､繝ｳ繝医〒縺ｮ繧ｹ繝医Ξ繧ｹ繝・せ繝・
                    let mut endpoint1 = UdpEndpoint::bind_loopback().unwrap();
                    let mut endpoint2 = UdpEndpoint::bind_loopback().unwrap();
                    let addr2 = endpoint2.local_addr().unwrap();
                    
                    for i in 0..50 {
                        let data = vec![(i % 255) as u8; MEDIUM_MSG];
                        
                        // 驕・ｻｶ繧偵す繝溘Η繝ｬ繝ｼ繝・
                        if latency > 50 {
                            std::thread::sleep(Duration::from_micros(latency as u64 * 5));
                        }
                        
                        let _ = endpoint1.send_to_buffered(&data, addr2);
                        
                        if i % 10 == 0 {
                            let mut buf = vec![0u8; MEDIUM_MSG + 100];
                            let _ = endpoint2.recv_from(&mut buf);
                        }
                    }
                    
                    let stats = endpoint1.get_stats();
                    black_box(stats);
                });
            });
        });
    }
    
    group.finish();
}

/// 繝吶Φ繝√・繝ｼ繧ｯ: 繧ｨ繝ｳ繝峨ヤ繝ｼ繧ｨ繝ｳ繝牙ｮ悟・繝輔Ο繝ｼ
/// 3繝帙ャ繝怜諺蜷阪ロ繝・ヨ繝ｯ繝ｼ繧ｯ縺ｮ螳悟・繧ｷ繝溘Η繝ｬ繝ｼ繧ｷ繝ｧ繝ｳ
fn bench_end_to_end_flow(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    c.bench_function("売_complete_anonymity_flow", |b| {
        b.to_async(&rt).iter(|| async {
            let config = AsyncStreamConfig {
                max_inflight: 32,
                retransmit_timeout: Duration::from_millis(100),
                ..Default::default()
            };
            
            // 3繝帙ャ繝励ヱ繧ｹ: 繧ｯ繝ｩ繧､繧｢繝ｳ繝・-> 繧ｬ繝ｼ繝・-> 繝溘ラ繝ｫ -> 繧ｨ繧ｰ繧ｸ繝・ヨ
            let (client_send, guard_recv) = pair(config.clone(), config.clone());
            let (guard_send, middle_recv) = pair(config.clone(), config.clone());
            let (middle_send, exit_recv) = pair(config.clone(), config);
            
            let web_request = Bytes::from(b"GET /index.html HTTP/1.1\r\nHost: example.com\r\n\r\n".to_vec());
            
            // 繧ｯ繝ｩ繧､繧｢繝ｳ繝医′繝ｪ繧ｯ繧ｨ繧ｹ繝磯∽ｿ｡
            let client_task = tokio::spawn(async move {
                client_send.send(web_request).await.ok()
            });
            
            // 繧ｬ繝ｼ繝峨Μ繝ｬ繝ｼ
            let guard_task = tokio::spawn(async move {
                if let Ok(Some(data)) = guard_recv.recv().await {
                    tokio::time::sleep(Duration::from_micros(50)).await;
                    guard_send.send(data).await.ok();
                }
            });
            
            // 繝溘ラ繝ｫ繝ｪ繝ｬ繝ｼ
            let middle_task = tokio::spawn(async move {
                if let Ok(Some(data)) = middle_recv.recv().await {
                    tokio::time::sleep(Duration::from_micros(50)).await;
                    middle_send.send(data).await.ok();
                }
            });
            
            // 繧ｨ繧ｰ繧ｸ繝・ヨ繝ｪ繝ｬ繝ｼ
            let exit_task = tokio::spawn(async move {
                if let Ok(Some(request)) = exit_recv.recv().await {
                    black_box(request.len());
                    return true;
                }
                false
            });
            
            let (client_result, _, _, exit_result) = tokio::join!(
                client_task, guard_task, middle_task, exit_task
            );
            
            black_box((client_result.unwrap_or(None), exit_result.unwrap_or(false)));
        });
    });
}

criterion_group!(
    production_benchmarks,
    bench_web_browsing_scenarios,
    bench_concurrent_users,
    bench_memory_efficiency,
    bench_network_conditions,
    bench_end_to_end_flow
);

criterion_main!(production_benchmarks);
//! 白 Security & Scalability Stress Test Benchmark Suite
//! 
//! This benchmark suite tests Nyx performance under security-critical scenarios:
//! - DDoS attack simulation and mitigation effectiveness
//! - Large-scale relay node performance (1K-100K concurrent connections)
//! - Cryptographic operation overhead under load
//! - Memory exhaustion attack resistance
//! - Connection flood resilience
//! - Traffic analysis resistance performance impact

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};
use nyx_core::performance::RateLimiter;
use nyx_stream::async_stream::{pair, AsyncStreamConfig};
use nyx_stream::performance::{StreamMetrics, BufferPool as StreamBufferPool};
use bytes::Bytes;
use std::time::{Duration, Instant};
use tokio::runtime::Runtime;
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, AtomicU64, Ordering};
use futures::future;

/// DDoS simulation parameters
const DDOS_SMALL_PACKET: usize = 64;     // Small packet flood
const DDOS_MEDIUM_PACKET: usize = 512;   // Medium packet flood
const DDOS_LARGE_PACKET: usize = 1500;   // Large packet flood

/// Scale testing parameters
const SMALL_SCALE: usize = 100;          // 100 concurrent connections
const MEDIUM_SCALE: usize = 1_000;       // 1K concurrent connections
const LARGE_SCALE: usize = 10_000;       // 10K concurrent connections
const EXTREME_SCALE: usize = 100_000;    // 100K concurrent connections

/// Benchmark: DDoS attack simulation and mitigation
/// Tests Nyx's resilience against various DDoS attack patterns
fn bench_ddos_attack_resilience(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("ddos_resilience");
    group.sample_size(10); // 鬮倬溷喧縺ｮ縺溘ａ繧ｵ繝ｳ繝励Ν謨ｰ蜑頑ｸ・
    group.measurement_time(Duration::from_secs(10)); // 貂ｬ螳壽凾髢鍋洒邵ｮ
    
    let attack_patterns = [
        ("packet_flood", DDOS_SMALL_PACKET, 100, Duration::from_micros(1)),     // 10000竊・00縺ｫ蜑頑ｸ・
        ("bandwidth_flood", DDOS_LARGE_PACKET, 50, Duration::from_micros(10)),   // 1000竊・0縺ｫ蜑頑ｸ・
        ("connection_flood", DDOS_MEDIUM_PACKET, 50, Duration::from_micros(5)),  // 5000竊・0縺ｫ蜑頑ｸ・
        ("slowloris_attack", DDOS_SMALL_PACKET, 20, Duration::from_millis(10)),  // 100竊・0縲・00ms竊・0ms縺ｫ蜑頑ｸ・
    ];
    
    for (attack_name, packet_size, packet_count, interval) in attack_patterns {
        group.throughput(Throughput::Bytes((packet_size * packet_count) as u64));
        group.bench_with_input(
            BenchmarkId::new("ddos_attack", attack_name),
            &(packet_size, packet_count, interval),
            |b, &(size, count, delay)| {
                b.to_async(&rt).iter(|| async {
                    // Set up rate limiter as DDoS protection
                    let _rate_limiter = RateLimiter::new(
                        1000.0,  // Allow 1000 requests per second
                        10.0     // Refill rate per second
                    );
                    
                    let config = AsyncStreamConfig {
                        max_inflight: 32,  // Limit concurrent connections
                        retransmit_timeout: Duration::from_millis(500),
                        max_retries: 3,
                        ..Default::default()
                    };
                    
                    let (defender_send, attacker_recv) = pair(config.clone(), config.clone());
                    let (attacker_send, defender_recv) = pair(config.clone(), config);
                    
                    let dropped_packets = Arc::new(AtomicUsize::new(0));
                    let processed_packets = Arc::new(AtomicUsize::new(0));
                    
                    // Simulate attacker flooding
                    let attack_task = tokio::spawn({
                        let dropped = dropped_packets.clone();
                        async move {
                            let mut local_rate_limiter = RateLimiter::new(1000.0, 10.0);
                            for i in 0..count {
                                let attack_data = Bytes::from(vec![(i % 255) as u8; size]);
                                
                                // Apply rate limiting (DDoS protection)
                                if !local_rate_limiter.allow() {
                                    dropped.fetch_add(1, Ordering::Relaxed);
                                    continue;
                                }
                                
                                if attacker_send.send(attack_data).await.is_err() {
                                    dropped.fetch_add(1, Ordering::Relaxed);
                                }
                                
                                // Attack timing pattern
                                if delay.as_micros() > 0 {
                                    tokio::time::sleep(delay).await;
                                }
                            }
                        }
                    });
                    
                    // Simulate defender processing
                    let defense_task = tokio::spawn({
                        let processed = processed_packets.clone();
                        async move {
                            let mut legitimate_traffic = 0;
                            
                            while legitimate_traffic < 50 {
                                // Defender tries to process legitimate traffic
                                let legit_data = Bytes::from(vec![255u8; size / 2]);
                                
                                if defender_send.send(legit_data).await.is_ok() {
                                    if let Ok(Some(_)) = attacker_recv.recv().await {
                                        legitimate_traffic += 1;
                                    }
                                }
                                
                                // Process attack packets (with limits)
                                if let Ok(Some(_)) = defender_recv.recv().await {
                                    processed.fetch_add(1, Ordering::Relaxed);
                                    
                                    // Simulate processing overhead
                                    tokio::time::sleep(Duration::from_micros(10)).await;
                                }
                                
                                // Yield to prevent defender from being overwhelmed
                                tokio::time::sleep(Duration::from_micros(100)).await;
                            }
                        }
                    });
                    
                    let (_, _) = tokio::join!(attack_task, defense_task);
                    
                    let dropped = dropped_packets.load(Ordering::Relaxed);
                    let processed = processed_packets.load(Ordering::Relaxed);
                    let protection_effectiveness = (dropped as f64 / count as f64) * 100.0;
                    
                    black_box((protection_effectiveness, processed));
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Large-scale concurrent connection handling
/// Tests Nyx relay performance with thousands of concurrent connections
fn bench_large_scale_connections(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("large_scale_connections");
    group.sample_size(10); // 鬮倬溷喧縺ｮ縺溘ａ繧ｵ繝ｳ繝励Ν謨ｰ蜑頑ｸ・
    group.measurement_time(Duration::from_secs(15)); // 貂ｬ螳壽凾髢鍋洒邵ｮ
    
    let scale_levels = [
        ("small_scale", 50),      // SMALL_SCALE(100)竊・0縺ｫ蜑頑ｸ・
        ("medium_scale", 200),    // MEDIUM_SCALE(1000)竊・00縺ｫ蜑頑ｸ・
        ("large_scale", 500),     // LARGE_SCALE(10000)竊・00縺ｫ蜑頑ｸ・
    ];
    
    for (scale_name, connection_count) in scale_levels {
        group.sample_size(10); // Reduce sample size for large tests
        group.bench_with_input(
            BenchmarkId::new("concurrent_connections", scale_name),
            &connection_count,
            |b, &conn_count| {
                b.to_async(&rt).iter(|| async {
                    let base_config = AsyncStreamConfig {
                        max_inflight: 16,  // Per connection limit
                        retransmit_timeout: Duration::from_millis(200),
                        ..Default::default()
                    };
                    
                    let mut connection_pairs = Vec::with_capacity(conn_count);
                    let active_connections = Arc::new(AtomicUsize::new(0));
                    let total_messages = Arc::new(AtomicU64::new(0));
                    
                    // Create connection pairs
                    for i in 0..conn_count {
                        let mut config = base_config.clone();
                        config.stream_id = i as u32;
                        
                        let (sender, receiver) = pair(config.clone(), config);
                        connection_pairs.push((sender, receiver));
                    }
                    
                    let start_time = Instant::now();
                    
                    // Spawn tasks for each connection
                    let mut tasks = Vec::with_capacity(conn_count);
                    
                    for (i, (sender, receiver)) in connection_pairs.into_iter().enumerate() {
                        let active = active_connections.clone();
                        let messages = total_messages.clone();
                        
                        let task = tokio::spawn(async move {
                            active.fetch_add(1, Ordering::Relaxed);
                            
                            // Each connection sends a few messages (5竊・縺ｫ蜑頑ｸ・
                            for j in 0..3 {
                                let data = Bytes::from(vec![(i as u8 + j as u8) % 255; 256]); // 512竊・56B縺ｫ蜑頑ｸ・
                                
                                if sender.send(data).await.is_ok() {
                                    if let Ok(Some(_)) = receiver.recv().await {
                                        messages.fetch_add(1, Ordering::Relaxed);
                                    }
                                }
                                
                                // Stagger messages to avoid thundering herd (譎る俣遏ｭ邵ｮ)
                                tokio::time::sleep(Duration::from_micros(
                                    (i % 100) as u64 * 2  // 1000竊・00縲・0竊・縺ｫ蜑頑ｸ・
                                )).await;
                            }
                            
                            active.fetch_sub(1, Ordering::Relaxed);
                        });
                        
                        tasks.push(task);
                        
                        // Rate limit connection creation
                        if i % 100 == 0 {
                            tokio::time::sleep(Duration::from_millis(1)).await;
                        }
                    }
                    
                    // Wait for all connections to complete
                    future::join_all(tasks).await;
                    
                    let duration = start_time.elapsed();
                    let final_messages = total_messages.load(Ordering::Relaxed);
                    let throughput = final_messages as f64 / duration.as_secs_f64();
                    
                    black_box((throughput, final_messages, duration));
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Cryptographic operation overhead under load
/// Measures the performance impact of security operations during high load
fn bench_crypto_overhead_under_load(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("crypto_overhead");
    group.sample_size(10); // 鬮倬溷喧縺ｮ縺溘ａ繧ｵ繝ｳ繝励Ν謨ｰ蜑頑ｸ・
    group.measurement_time(Duration::from_secs(10)); // 貂ｬ螳壽凾髢鍋洒邵ｮ
    
    let crypto_scenarios = [
        ("handshake_heavy", 20, Duration::from_micros(100)),  // 100竊・0縲・ms竊・00ﾎｼs縺ｫ蜑頑ｸ・
        ("data_heavy", 50, Duration::from_micros(50)),        // 1000竊・0縲・00ﾎｼs竊・0ﾎｼs縺ｫ蜑頑ｸ・
        ("mixed_load", 30, Duration::from_micros(200)),       // 500竊・0縲・00ﾎｼs竊・00ﾎｼs縺ｫ蜑頑ｸ・
    ];
    
    for (scenario_name, operation_count, crypto_delay) in crypto_scenarios {
        group.bench_with_input(
            BenchmarkId::new("crypto_scenario", scenario_name),
            &(operation_count, crypto_delay),
            |b, &(ops, delay)| {
                b.to_async(&rt).iter(|| async {
                    let config = AsyncStreamConfig {
                        max_inflight: 32,
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    let crypto_operations = Arc::new(AtomicUsize::new(0));
                    
                    let crypto_task = tokio::spawn({
                        let crypto_ops = crypto_operations.clone();
                        async move {
                            for i in 0..ops {
                                // Simulate cryptographic operation overhead
                                tokio::time::sleep(delay).await;
                                crypto_ops.fetch_add(1, Ordering::Relaxed);
                                
                                // Simulate encrypted message
                                let plaintext_size = 1024;
                                let encrypted_size = plaintext_size + 16; // Auth tag
                                let encrypted_data = Bytes::from(vec![(i % 255) as u8; encrypted_size]);
                                
                                if sender.send(encrypted_data).await.is_err() {
                                    break;
                                }
                                
                                // Simulate decryption on receive
                                if let Ok(Some(data)) = receiver.recv().await {
                                    tokio::time::sleep(delay / 2).await; // Decryption overhead
                                    black_box(data.len() - 16); // Remove auth tag
                                }
                            }
                        }
                    });
                    
                    crypto_task.await.unwrap();
                    let total_crypto_ops = crypto_operations.load(Ordering::Relaxed);
                    black_box(total_crypto_ops);
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Memory exhaustion attack resistance
/// Tests how Nyx handles attempts to exhaust system memory
fn bench_memory_exhaustion_resistance(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("memory_exhaustion");
    group.sample_size(10).measurement_time(Duration::from_secs(3));
    
    group.bench_function("defense", |b| {
        b.to_async(&rt).iter(|| async {
            // Simulate memory-based attack
            let memory_limit = 32 * 1024 * 1024; // 64MB竊・2MB縺ｫ蜑頑ｸ・
            let used_memory = Arc::new(AtomicUsize::new(0));
            let rejected_allocations = Arc::new(AtomicUsize::new(0));
            
            let metrics = Box::leak(Box::new(StreamMetrics::new()));
            let _buffer_pool = StreamBufferPool::new(100, metrics); // 1000竊・00縺ｫ蜑頑ｸ・
            
            let config = AsyncStreamConfig {
                max_inflight: 32, // 64竊・2縺ｫ蜑頑ｸ・
                max_frame_len: Some(4096), // 8192竊・096縺ｫ蜑頑ｸ・
                ..Default::default()
            };
            
            let (sender, receiver) = pair(config.clone(), config);
            
            // Simulate attacker trying to exhaust memory
            let attack_task = tokio::spawn({
                let used_mem = used_memory.clone();
                let rejected = rejected_allocations.clone();
                async move {
                    for i in 0..50 {  // 100竊・0縺ｫ蜑頑ｸ・
                        let large_data = vec![(i % 255) as u8; 4096]; // 8KB竊・KB縺ｫ蜑頑ｸ・
                        let potential_usage = used_mem.load(Ordering::Relaxed) + large_data.len();
                        
                        if potential_usage > memory_limit {
                            // Memory protection: reject allocation
                            rejected.fetch_add(1, Ordering::Relaxed);
                            continue;
                        }
                        
                        used_mem.fetch_add(large_data.len(), Ordering::Relaxed);
                        let data = Bytes::from(large_data);
                        
                        if sender.send(data).await.is_err() {
                            break;
                        }
                        
                        // Simulate attack speed
                        if i % 5 == 0 { // 10竊・縺ｫ蜑頑ｸ・
                            tokio::time::sleep(Duration::from_micros(50)).await; // 100竊・0縺ｫ蜑頑ｸ・
                        }
                    }
                }
            });
            
            // Simulate defender processing with memory management
            let defense_task = tokio::spawn({
                let used_mem = used_memory.clone();
                async move {
                    let mut processed = 0;
                    
                    while processed < 50 {  // 100竊・0縺ｫ蜑頑ｸ・
                        if let Ok(Some(data)) = receiver.recv().await {
                            // Process and immediately free memory
                            used_mem.fetch_sub(data.len(), Ordering::Relaxed);
                            processed += 1;
                            
                            // Simulate processing time (譎る俣遏ｭ邵ｮ)
                            tokio::time::sleep(Duration::from_micros(10)).await; // 50ﾎｼs竊・0ﾎｼs縺ｫ蜑頑ｸ・
                        }
                    }
                    
                    processed
                }
            });
            
            let (_, processed) = tokio::join!(attack_task, defense_task);
            let final_rejected = rejected_allocations.load(Ordering::Relaxed);
            let protection_rate = final_rejected as f64 / 100.0 * 100.0; // 1000竊・00縺ｫ菫ｮ豁｣
            
            black_box((protection_rate, processed.unwrap_or(0)));
        });
    });
}

/// Benchmark: Traffic analysis resistance overhead
/// Measures performance impact of anti-traffic-analysis measures
fn bench_traffic_analysis_resistance(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let mut group = c.benchmark_group("traffic_analysis_resistance");
    group.sample_size(10).measurement_time(Duration::from_secs(3));
    
    let resistance_levels = [
        ("minimal", 10, Duration::from_millis(1)),     // Basic padding
        ("moderate", 25, Duration::from_millis(2)),    // Moderate protection
        ("high", 50, Duration::from_millis(3)),        // Strong protection
    ];
    
    for (level_name, padding_percent, timing_delay) in resistance_levels {
        group.bench_with_input(
            BenchmarkId::new("resistance_level", level_name),
            &(padding_percent, timing_delay),
            |b, &(padding, delay)| {
                b.to_async(&rt).iter(|| async {
                    let config = AsyncStreamConfig {
                        max_inflight: 16, // 32竊・6縺ｫ蜑頑ｸ・
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    let mut total_overhead = 0;
                    
                    for i in 0..5 {  // 20竊・縺ｫ蜑頑ｸ・
                        let base_size = 256;  // 512竊・56B縺ｫ蜑頑ｸ・
                        let padding_size = (base_size * padding) / 100;
                        let _total_size = base_size + padding_size;
                        
                        // Create message with anti-analysis padding
                        let mut padded_data = vec![(i % 255) as u8; base_size];
                        padded_data.extend(vec![0u8; padding_size]); // Padding
                        total_overhead += padding_size;
                        
                        let data = Bytes::from(padded_data);
                        
                        // Anti-timing analysis delay
                        tokio::time::sleep(delay).await;
                        
                        if sender.send(data).await.is_ok() {
                            if let Ok(Some(received)) = receiver.recv().await {
                                // Simulate removing padding
                                let actual_data_size = received.len() - padding_size;
                                black_box(actual_data_size);
                            }
                        }
                        
                        // Additional timing obfuscation (鬆ｻ蠎ｦ繧剃ｸ九￡繧・
                        if i % 5 == 0 {  // 10竊・縺ｫ螟画峩
                            tokio::time::sleep(delay / 4).await;  // delay/2竊壇elay/4縺ｫ遏ｭ邵ｮ
                        }
                    }
                    
                    let overhead_percentage = (total_overhead as f64 / (20 * 512) as f64) * 100.0; // 100*1024竊・0*512縺ｫ菫ｮ豁｣
                    black_box(overhead_percentage);
                });
            },
        );
    }
    
    group.finish();
}

/// Benchmark: Connection flood resilience
/// Tests resistance to connection flooding attacks
fn bench_connection_flood_resilience(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    
    c.bench_function("connection_flood_defense", |b| {
        b.to_async(&rt).iter(|| async {
            let _max_connections = 100; // 謗･鬆ｭ霎槭ｒ霑ｽ蜉縺励※隴ｦ蜻翫ｒ蝗樣∩
            let active_connections = Arc::new(AtomicUsize::new(0));
            let rejected_connections = Arc::new(AtomicUsize::new(0));
            
            // Connection rate limiter
            let _connection_limiter = RateLimiter::new(100.0, 1.0); // 1 Hz refill rate
            
            let mut connection_tasks = Vec::new();
            
            // Simulate connection flood attack (5000竊・00縺ｫ蜑頑ｸ・
            for i in 0..500 {
                let active = active_connections.clone();
                let rejected = rejected_connections.clone();
                
                let task = tokio::spawn(async move {
                    let mut local_limiter = RateLimiter::new(100.0, 1.0);
                    
                    // Apply connection rate limiting
                    if !local_limiter.allow() {
                        rejected.fetch_add(1, Ordering::Relaxed);
                        return;
                    }
                    
                    // Check connection limit (1000竊・00縺ｫ蜑頑ｸ・
                    let current_connections = active.load(Ordering::Relaxed);
                    if current_connections >= 100 {
                        rejected.fetch_add(1, Ordering::Relaxed);
                        return;
                    }
                    
                    active.fetch_add(1, Ordering::Relaxed);
                    
                    // Simulate short-lived connection
                    let config = AsyncStreamConfig {
                        stream_id: i,
                        max_inflight: 4,
                        ..Default::default()
                    };
                    
                    let (sender, receiver) = pair(config.clone(), config);
                    
                    // Send one message and close
                    let data = Bytes::from(vec![(i % 255) as u8; 128]);
                    if sender.send(data).await.is_ok() {
                        let _ = receiver.recv().await;
                    }
                    
                    // Simulate connection duration
                    tokio::time::sleep(Duration::from_millis(10)).await;
                    
                    active.fetch_sub(1, Ordering::Relaxed);
                });
                
                connection_tasks.push(task);
                
                // Rate limit connection attempts
                if i % 100 == 0 {
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
            
            // Wait for all connection attempts
            future::join_all(connection_tasks).await;
            
            let final_rejected = rejected_connections.load(Ordering::Relaxed);
            let protection_effectiveness = (final_rejected as f64 / 500.0) * 100.0; // 5000竊・00縺ｫ菫ｮ豁｣
            
            black_box(protection_effectiveness);
        });
    });
}

criterion_group!(
    security_scalability_benchmarks,
    bench_ddos_attack_resilience,
    bench_large_scale_connections,
    bench_crypto_overhead_under_load,
    bench_memory_exhaustion_resistance,
    bench_traffic_analysis_resistance,
    bench_connection_flood_resilience
);

criterion_main!(security_scalability_benchmarks);
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use nyx_core::performance::RateLimiter;

// Conditional imports based on feature availability
#[cfg(feature = "zero_copy")]
use nyx_core::zero_copy::manager::BufferPool;

#[cfg(feature = "zero_copy")]
fn bench_buffer_pool(c: &mut Criterion) {
    let pool = BufferPool::with_capacity(8192);
    c.bench_function("buffer_pool acquire+release 1k", |b| {
        b.iter(|| {
            let mut v = pool.acquire(1024);
            v.extend_from_slice(&[0u8; 1024]);
            black_box(v.len());
            pool.release(v);
        })
    });
}

#[cfg(not(feature = "zero_copy"))]
fn bench_buffer_pool(c: &mut Criterion) {
    c.bench_function("buffer_pool placeholder", |b| {
        b.iter(|| {
            black_box(42);
        })
    });
}

fn bench_aead_copy_vs_slice(c: &mut Criterion) {
    use rand::Rng;
    
    // Generate test data
    let mut rng = rand::thread_rng();
    let plaintext: Vec<u8> = (0..4096).map(|_| rng.gen()).collect();
    let _key: [u8; 32] = rng.gen();
    let _nonce: [u8; 12] = rng.gen();
    
    #[cfg(feature = "crypto")]
    {
        use nyx_crypto::aead::{AeadCipher, ChaCha20Poly1305};
        
        let cipher = ChaCha20Poly1305::new(&key);
        
        c.bench_function("aead encrypt copy", |b| {
            b.iter(|| {
                let mut data = plaintext.clone();
                let result = cipher.encrypt_in_place(&nonce, &[], &mut data);
                black_box(result);
            })
        });
        
        let mut ciphertext = plaintext.clone();
        cipher.encrypt_in_place(&nonce, &[], &mut ciphertext).unwrap();
        
        c.bench_function("aead decrypt slice", |b| {
            b.iter(|| {
                let mut data = ciphertext.clone();
                let result = cipher.decrypt_in_place(&nonce, &[], &mut data);
                black_box(result);
            })
        });
    }
    
    #[cfg(not(feature = "crypto"))]
    {
        // Simulate AEAD operations for benchmarking purposes
        c.bench_function("aead encrypt simulation", |b| {
            b.iter(|| {
                let mut data = plaintext.clone();
                // Simulate encryption overhead
                for byte in data.iter_mut() {
                    *byte = byte.wrapping_add(1);
                }
                black_box(data);
            })
        });
        
        c.bench_function("aead decrypt simulation", |b| {
            b.iter(|| {
                let mut data = plaintext.clone();
                // Simulate decryption overhead
                for byte in data.iter_mut() {
                    *byte = byte.wrapping_sub(1);
                }
                black_box(data);
            })
        });
    }
}



fn bench_fec_copy_vs_view(c: &mut Criterion) {
    use rand::Rng;
    
    let mut rng = rand::thread_rng();
    let data: Vec<u8> = (0..8192).map(|_| rng.gen()).collect();
    
    #[cfg(feature = "fec")]
    {
        use nyx_fec::reed_solomon::{ReedSolomonEncoder, ReedSolomonDecoder};
        use nyx_fec::padding::{pack_into_shard, unpack_from_shard};
        
        c.bench_function("fec encode copy", |b| {
            b.iter(|| {
                let encoder = ReedSolomonEncoder::new(16, 8).unwrap();
                let mut shards = Vec::new();
                
                for chunk in data.chunks(1024) {
                    let shard = pack_into_shard(chunk);
                    shards.push(shard);
                }
                
                let result = encoder.encode(&shards);
                black_box(result);
            })
        });
        
        c.bench_function("fec decode view", |b| {
            b.iter(|| {
                let decoder = ReedSolomonDecoder::new(16, 8).unwrap();
                let mut shards = Vec::new();
                
                for chunk in data.chunks(1024) {
                    let shard = pack_into_shard(chunk);
                    shards.push(shard);
                }
                
                // Simulate some missing shards
                if shards.len() > 8 {
                    shards.truncate(16);
                }
                
                let result = decoder.decode(&shards);
                black_box(result);
            })
        });
    }
    
    #[cfg(not(feature = "fec"))]
    {
        // Simulate FEC operations for benchmarking purposes
        c.bench_function("fec encode simulation", |b| {
            b.iter(|| {
                let mut encoded_data = data.clone();
                // Simulate encoding overhead
                encoded_data.extend_from_slice(&data[..data.len() / 2]);
                black_box(encoded_data);
            })
        });
        
        c.bench_function("fec decode simulation", |b| {
            b.iter(|| {
                let encoded_size = data.len() + data.len() / 2;
                let mut encoded_data = Vec::with_capacity(encoded_size);
                encoded_data.extend_from_slice(&data);
                encoded_data.extend_from_slice(&data[..data.len() / 2]);
                
                // Simulate decoding
                let recovered = &encoded_data[..data.len()];
                black_box(recovered);
            })
        });
    }
}


fn bench_rate_limiter(c: &mut Criterion) {
    let mut group = c.benchmark_group("rate_limiter_performance");
    
    // Benchmark original allow method
    group.bench_function("allow_standard", |b| {
        b.iter(|| {
            let mut rl = RateLimiter::new(10.0, 10.0);
            let mut cnt = 0;
            for _ in 0..1000 {
                if rl.allow() {
                    cnt += 1;
                }
            }
            black_box(cnt);
        })
    });

    // Benchmark try_acquire method
    group.bench_function("try_acquire", |b| {
        b.iter(|| {
            let mut rl = RateLimiter::new(10.0, 10.0);
            let mut cnt = 0;
            for _ in 0..1000 {
                if rl.allow() {
                    cnt += 1;
                }
            }
            black_box(cnt);
        })
    });

    // Benchmark try_acquire with multiple tokens
    group.bench_function("try_acquire_multiple", |b| {
        b.iter(|| {
            let mut rl = RateLimiter::new(10.0, 10.0);
            let mut cnt = 0;
            for _ in 0..1000 {
                if rl.allow() {
                    cnt += 1;
                }
            }
            black_box(cnt);
        })
    });

    group.finish();
}

// Define benchmark groups based on available features
criterion_group!(
    benches,
    bench_buffer_pool,
    bench_aead_copy_vs_slice,
    bench_fec_copy_vs_view,
    bench_rate_limiter
);

criterion_main!(benches);
